term1=kappa_*alpha*Z[1]+sigma*Z[2]
dmvnorm(Z)* drho(-kappa_*Z[1])*(term1-prox_rho(term1,gamma_))^2
}
f2<-function(Z,kappa_,alpha,sigma,gamma_){
dmvnorm(Z)* d2rho(-kappa_*Z[1])*prox_rho(kappa_*alpha*Z[1]+sigma*Z[2],gamma_)
}
f3<-function(Z,kappa_,alpha,sigma,gamma_){
dmvnorm(Z)* 2*drho(-kappa_*Z[1])/(1+gamma_*d2rho(prox_rho(kappa_*alpha*Z[1]+sigma*Z[2],gamma_)))
}
S_minus_V<-function(v,delta=3,kappa_=1,lambda=0.8,intergal_limit=50){
y<- numeric(3)
sigma=v[1];alpha=v[2];gamma_=v[3]
y[1]=sqrt(2*delta*hcubature(f1,lowerLimit = c(-intergal_limit,-intergal_limit),
upperLimit = c(intergal_limit,intergal_limit),kappa_,alpha,sigma,gamma_)$integral)-sigma
y[2]=-2*delta*hcubature(f2,lowerLimit = c(-intergal_limit,-intergal_limit),
upperLimit = c(intergal_limit,intergal_limit),kappa_,alpha,sigma,gamma_)$integral-alpha
y[3]=(hcubature(f3,lowerLimit = c(-intergal_limit,-intergal_limit),
upperLimit = c(intergal_limit,intergal_limit),kappa_,alpha,sigma,gamma_)$integral-1+1/delta)/lambda  -gamma_
return(y)
}
delta=4
############### ridge with lambda=1/p
kappa1_seq=seq(0.2,2,by=0.2)
solution_three=matrix(0,nr=length(kappa1_seq),nc=3)
for(i in 1:length(kappa1_seq)){
kappa1=kappa1_seq[i]
v_init=c(0.7438824, 0.6313888, 0.917302)
solution_three[i,]=multiroot(S_minus_V,v_init ,delta=delta,kappa_=kappa1,lambda=lambda_sequence[jj],intergal_limit=20)$root
}
delta=4
############### ridge with lambda=1/p
kappa1_seq=seq(0.2,2,by=0.2)
solution_three=matrix(0,nr=length(kappa1_seq),nc=3)
for(i in 1:length(kappa1_seq)){
kappa1=kappa1_seq[i]
v_init=c(0.7438824, 0.6313888, 0.917302)
solution_three[i,]=multiroot(S_minus_V,v_init ,delta=delta,kappa_=kappa1,lambda=1,intergal_limit=20)$root
}
solution_three
i
kappa1=kappa1_seq[i]
v_init=c(0.1826211, 0.1243923, 0.2029217)
solution_three[i,]=multiroot(S_minus_V,v_init ,delta=delta,kappa_=kappa1,lambda=1,intergal_limit=20)$root
for(i in 1:length(kappa1_seq)){
kappa1=kappa1_seq[i]
v_init=c(0.1826211, 0.1243923, 0.2029217)
solution_three[i,]=multiroot(S_minus_V,v_init ,delta=delta,kappa_=kappa1,lambda=1,intergal_limit=20)$root
}
solution_three
# compute the classification error based on solution_three
return_class_err<-function(sigma,kappa1,alpha){
vv=numeric()
for(j in 1:10){
Z_matrix=matrix(rnorm(10000),nc=2)
vv[j]=mean(sapply(1:5000,function(ii){
Z1=Z_matrix[ii,1]
Z2=Z_matrix[ii,2]
drho(kappa1*Z2)*(sigma*Z1+alpha*kappa1*Z2<0)+(1-drho(kappa1*Z2))*(sigma*Z1+alpha*kappa1*Z2>0)
}))
}
mean(vv)
}
function_return_class_err_ridge=function(solution_three,kappa1_seq){
sapply(1:nrow(solution_three),function(ii){
kappa1=kappa1_seq[ii]
sigma=solution_three[ii,1]
alpha=solution_three[ii,2]
return_class_err(sigma,kappa1,alpha)
})
}
plot(kappa1_seq,function_return_class_err_ridge(solution_three,kappa1_seq),type="l")
plot(kappa1_seq,function_return_class_err_ridge(solution_three,kappa1_seq),type="l","class error")
plot(kappa1_seq,function_return_class_err_ridge(solution_three,kappa1_seq),type="l",main="class error")
plot(kappa1_seq,function_return_class_err_ridge(solution_three,kappa1_seq),type="l",ylab="class error")
sample(1:10,5)
sample(1:10,5,replace = FALSE)
sample(1:10,4,replace = FALSE)
p=250
kappa1=1
delta=8
n=p*delta
set.seed(1)
beta_true=rnorm(p,sd=kappa1)
X=matrix(rnorm(p*n,mean=0,sd=sqrt(1/p)),nc=p)
y1=rbinom(n,1, 1/(1+exp(-X%*%beta_true)))
test_index=sample(1:n,n/2,replace = FALSE)
fit_ridge<-glmnet(X[-test_index,],y1[-test_index],family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
# now the question is: how to estimate classification error use data
# we first try delta =8 and sample halve spliting
library(glmnet)
fit_ridge<-glmnet(X[-test_index,],y1[-test_index],family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
betahat_ridge=as.numeric(fit_ridge$beta)
ypred_ridge=1*(drho(X[test_index,]%*%betahat_ridge)>0.5)
mean(ypred_ridge!=y1[test_index])
abline(h=0.412,col=2)
abline(v=1,col=3)
cl_error_rep=rep(0,50)
for(rep_index in 1:50){
test_index=sample(1:n,n/2,replace = FALSE)
fit_ridge<-glmnet(X[-test_index,],y1[-test_index],family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
betahat_ridge=as.numeric(fit_ridge$beta)
ypred_ridge=1*(drho(X[test_index,]%*%betahat_ridge)>0.5)
cl_error_rep[rep_index]=mean(ypred_ridge!=y1[test_index])
}
mean(cl_error_rep)
abline(h=mean(cl_error_rep))
cl_error_rep=rep(0,50)
for(rep_index in 1:50){
test_index=sample(1:n,n/2,replace = FALSE)
fit_ridge<-glmnet(X[-test_index,],y1[-test_index],family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
betahat_ridge=as.numeric(fit_ridge$beta)
ypred_ridge=1*(drho(X[test_index,]%*%betahat_ridge)>0.5)
cl_error_rep[rep_index]=mean(ypred_ridge!=y1[test_index])
}
mean(cl_error_rep)
cl_error_rep=rep(0,100)
for(rep_index in 1:100){
test_index=sample(1:n,n/2,replace = FALSE)
fit_ridge<-glmnet(X[-test_index,],y1[-test_index],family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
betahat_ridge=as.numeric(fit_ridge$beta)
ypred_ridge=1*(drho(X[test_index,]%*%betahat_ridge)>0.5)
cl_error_rep[rep_index]=mean(ypred_ridge!=y1[test_index])
}
mean(cl_error_rep)
abline(h=mean(cl_error_rep))
error=rep(0,20)
for(rep_index in 1:20){
set.seed(rep_index)
beta_true=rnorm(p,sd=kappa1)
X=matrix(rnorm(p*n,mean=0,sd=sqrt(1/p)),nc=p)
y1=rbinom(n,1, 1/(1+exp(-X%*%beta_true)))
cl_error_rep=rep(0,50)
for(resample_index in 1:50){
test_index=sample(1:n,n/2,replace = FALSE)
fit_ridge<-glmnet(X[-test_index,],y1[-test_index],family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
betahat_ridge=as.numeric(fit_ridge$beta)
ypred_ridge=1*(drho(X[test_index,]%*%betahat_ridge)>0.5)
cl_error_rep[resample_index]=mean(ypred_ridge!=y1[test_index])
}
error[rep_index]=mean(cl_error_rep)
}
mean(error)
abline(h=mean(error))
p=250
kappa1=1
delta=5
n=p*delta
error=rep(0,20)
for(rep_index in 1:20){
set.seed(rep_index)
beta_true=rnorm(p,sd=kappa1)
X=matrix(rnorm(p*n,mean=0,sd=sqrt(1/p)),nc=p)
y1=rbinom(n,1, 1/(1+exp(-X%*%beta_true)))
cl_error_rep=rep(0,50)
for(resample_index in 1:50){
test_index=sample(1:n,n/5,replace = FALSE)
fit_ridge<-glmnet(X[-test_index,],y1[-test_index],family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
betahat_ridge=as.numeric(fit_ridge$beta)
ypred_ridge=1*(drho(X[test_index,]%*%betahat_ridge)>0.5)
cl_error_rep[resample_index]=mean(ypred_ridge!=y1[test_index])
}
error[rep_index]=mean(cl_error_rep)
}
mean(error)
set.seed(1)
beta_true=rnorm(p,sd=kappa1)
X=matrix(rnorm(p*n,mean=0,sd=sqrt(1/p)),nc=p)
y1=rbinom(n,1, 1/(1+exp(-X%*%beta_true)))
test_index=sample(1:n,n/2,replace = FALSE)
fit_ridge<-glmnet(X[-test_index,],y1[-test_index],family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
betahat_ridge=as.numeric(fit_ridge$beta)
ypred_ridge=1*(drho(X[test_index,]%*%betahat_ridge)>0.5)
mean(ypred_ridge!=y1[test_index])
test_index=sample(1:n,n/5,replace = FALSE)
fit_ridge<-glmnet(X[-test_index,],y1[-test_index],family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
betahat_ridge=as.numeric(fit_ridge$beta)
ypred_ridge=1*(drho(X[test_index,]%*%betahat_ridge)>0.5)
mean(ypred_ridge!=y1[test_index])
cl_error_rep=rep(0,50)
for(resample_index in 1:50){
test_index=sample(1:n,n/5,replace = FALSE)
fit_ridge<-glmnet(X[-test_index,],y1[-test_index],family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
betahat_ridge=as.numeric(fit_ridge$beta)
ypred_ridge=1*(drho(X[test_index,]%*%betahat_ridge)>0.5)
cl_error_rep[resample_index]=mean(ypred_ridge!=y1[test_index])
}
mean(cl_error_rep)
error
#plot(kappa1_seq,function_return_class_err_ridge(solution_three,kappa1_seq),type="l",ylab="class error")
theory_class_error=function_return_class_err_ridge(solution_three,kappa1_seq)
theory_class_error
mean(error)
which.min(abs(0.38-theory_class_error))
kappa1_seq[which.min(abs(0.38-theory_class_error))]
# now the question is: how to estimate classification error use data
library(glmnet)
empirical_kappa_hat_cv_onetime<-function(X,y1,theory_class_error,kappa1_seq,reample_time=50){
cl_error_rep=rep(0,50)
for(resample_index in 1:50){
test_index=sample(1:n,n/5,replace = FALSE)
fit_ridge<-glmnet(X[-test_index,],y1[-test_index],family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
betahat_ridge=as.numeric(fit_ridge$beta)
ypred_ridge=1*(drho(X[test_index,]%*%betahat_ridge)>0.5)
cl_error_rep[resample_index]=mean(ypred_ridge!=y1[test_index])
}
em_err=mean(cl_error_rep)
kappa1_seq[which.min(abs(em_err-theory_class_error))]
}
######
load("result_matrix_5")
load("theory_class_error_5")
kappa1_seq=seq(0.2,2,by=0.05)
plot(kappa1_seq,apply(result_matrix,1,mean),main = "delta = 5 ")
lines(kappa1_seq,kappa1_seq,col=2)
plot(kappa1_seq,apply(result_matrix,1,mean),main = "delta = 5 ",ylab = "estimated kappa1")
lines(kappa1_seq,kappa1_seq,col=2)
load("result_matrix_8")
load("theory_class_error_8")
kappa1_seq=seq(0.2,2,by=0.05)
plot(kappa1_seq,apply(result_matrix,1,mean),main = "delta = 8 ",ylab = "estimated kappa1")
lines(kappa1_seq,kappa1_seq,col=2)
######
par(mfrow=c(1,2))
load("result_matrix_5")
load("theory_class_error_5")
kappa1_seq=seq(0.2,2,by=0.05)
plot(kappa1_seq,apply(result_matrix,1,mean),main = "delta = 5 ",ylab = "estimated kappa1")
lines(kappa1_seq,kappa1_seq,col=2)
load("result_matrix_8")
load("theory_class_error_8")
kappa1_seq=seq(0.2,2,by=0.05)
plot(kappa1_seq,apply(result_matrix,1,mean),main = "delta = 8 ",ylab = "estimated kappa1")
lines(kappa1_seq,kappa1_seq,col=2)
rm(list=ls())
library(parallel)
numCores=detectCores()
library(sigmoid)
library(cubature) # numerical integration
library(mvtnorm)
library(rootSolve) # root finding by built in function
rho<-function(x){
if(x>10) return(10.00005)
log(1+exp(x))
}
drho<-function(x){
sigmoid(x)
}
d2rho<-function(x){
sigmoid(x)*(1-sigmoid(x))
}
prox_rho<-function(v,t){          # argmin_x t*pho(x)+|v-x|^2/2
return(optim(0,function(x){(x-v)^2/2+t*rho(x)},method = "Brent",lower = -1000000000,upper = 1000000000)$par)
newf<-function(xx){
xx+t*drho(xx)-v
}
uniroot(newf,c(-10000,10000))$root
}
f1<-function(Z,kappa_,alpha,sigma,gamma_){
term1=kappa_*alpha*Z[1]+sigma*Z[2]
dmvnorm(Z)* drho(-kappa_*Z[1])*(term1-prox_rho(term1,gamma_))^2
}
f2<-function(Z,kappa_,alpha,sigma,gamma_){
dmvnorm(Z)* d2rho(-kappa_*Z[1])*prox_rho(kappa_*alpha*Z[1]+sigma*Z[2],gamma_)
}
f3<-function(Z,kappa_,alpha,sigma,gamma_){
dmvnorm(Z)* 2*drho(-kappa_*Z[1])/(1+gamma_*d2rho(prox_rho(kappa_*alpha*Z[1]+sigma*Z[2],gamma_)))
}
S_minus_V<-function(v,delta=3,kappa_=1,lambda=0.8,intergal_limit=50){
y<- numeric(3)
sigma=v[1];alpha=v[2];gamma_=v[3]
y[1]=sqrt(2*delta*hcubature(f1,lowerLimit = c(-intergal_limit,-intergal_limit),
upperLimit = c(intergal_limit,intergal_limit),kappa_,alpha,sigma,gamma_)$integral)-sigma
y[2]=-2*delta*hcubature(f2,lowerLimit = c(-intergal_limit,-intergal_limit),
upperLimit = c(intergal_limit,intergal_limit),kappa_,alpha,sigma,gamma_)$integral-alpha
y[3]=(hcubature(f3,lowerLimit = c(-intergal_limit,-intergal_limit),
upperLimit = c(intergal_limit,intergal_limit),kappa_,alpha,sigma,gamma_)$integral-1+1/delta)/lambda  -gamma_
return(y)
}
#  how about we have p greater than n, can we use ridge to detect kesee
kappa2=2
kappa1=1
delta=1/2
m=3
kesee=0.6
v_init=c(0.1826211, 0.1243923, 0.2029217)
multiroot(S_minus_V,v_init ,delta=delta,kappa_=kappa1,lambda=1,intergal_limit=20)$root
# ridge for target data
v_init=c(0.4314904, 0.1264642, 1.7243201)
# ridge for source data
multiroot(S_minus_V,v_init ,delta=delta*m,kappa_=kappa1,lambda=1,intergal_limit=20)$root
# ridge for source data
multiroot(S_minus_V,v_init ,delta=delta*m,kappa_=kappa2,lambda=1,intergal_limit=20)$root
# assume known signal strength, directly inner product two MLE on two datasets and divide (alpha1*alpha2*kappa1*kappa2)
library(glmnet)
kesee=0.6
n=p*delta
NUM_repeat=50
p=200
n=p*delta
NUM_repeat=50
inner_prod=c()
for(rep_index in 1:NUM_repeat){
set.seed(rep_index)
M=m*n
beta_true=rnorm(p,sd=kappa1)
X=matrix(rnorm(p*n,mean=0,sd=sqrt(1/p)),nc=p)
Xstar=matrix(rnorm(p*M,mean=0,sd=sqrt(1/p)),nc=p)
y1=rbinom(n,1, 1/(1+exp(-X%*%beta_true)))
lambda=kappa2*(sqrt(1-kesee^2))
beta0=kesee*kappa2/kappa1*beta_true+lambda*rnorm(p)
y2=rbinom(M,1, 1/(1+exp(-Xstar%*%beta0)))
fit_mle_real<-glmnet(X,y1,family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
fit_mle_hist<-glmnet(Xstar,y2,family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
inner_prod=c(inner_prod,sum(as.numeric(fit_mle_real$beta)*as.numeric(fit_mle_hist$beta))/p)
}
mean(inner_prod)/0.1264642/0.1103106/kappa1/kappa2
p=250
n=p*delta
NUM_repeat=50
inner_prod=c()
for(rep_index in 1:NUM_repeat){
set.seed(rep_index)
M=m*n
beta_true=rnorm(p,sd=kappa1)
X=matrix(rnorm(p*n,mean=0,sd=sqrt(1/p)),nc=p)
Xstar=matrix(rnorm(p*M,mean=0,sd=sqrt(1/p)),nc=p)
y1=rbinom(n,1, 1/(1+exp(-X%*%beta_true)))
lambda=kappa2*(sqrt(1-kesee^2))
beta0=kesee*kappa2/kappa1*beta_true+lambda*rnorm(p)
y2=rbinom(M,1, 1/(1+exp(-Xstar%*%beta0)))
fit_mle_real<-glmnet(X,y1,family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
fit_mle_hist<-glmnet(Xstar,y2,family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
inner_prod=c(inner_prod,sum(as.numeric(fit_mle_real$beta)*as.numeric(fit_mle_hist$beta))/p)
}
mean(inner_prod)/0.1264642/0.1103106/kappa1/kappa2
NUM_repeat=100
inner_prod=c()
for(rep_index in 1:NUM_repeat){
set.seed(rep_index)
M=m*n
beta_true=rnorm(p,sd=kappa1)
X=matrix(rnorm(p*n,mean=0,sd=sqrt(1/p)),nc=p)
Xstar=matrix(rnorm(p*M,mean=0,sd=sqrt(1/p)),nc=p)
y1=rbinom(n,1, 1/(1+exp(-X%*%beta_true)))
lambda=kappa2*(sqrt(1-kesee^2))
beta0=kesee*kappa2/kappa1*beta_true+lambda*rnorm(p)
y2=rbinom(M,1, 1/(1+exp(-Xstar%*%beta0)))
fit_mle_real<-glmnet(X,y1,family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
fit_mle_hist<-glmnet(Xstar,y2,family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
inner_prod=c(inner_prod,sum(as.numeric(fit_mle_real$beta)*as.numeric(fit_mle_hist$beta))/p)
}
mean(inner_prod)/0.1264642/0.1103106/kappa1/kappa2
m=6
# ridge for source data
multiroot(S_minus_V,v_init ,delta=delta*m,kappa_=kappa2,lambda=1,intergal_limit=20)$root
p=250
n=p*delta
NUM_repeat=100
inner_prod=c()
for(rep_index in 1:NUM_repeat){
set.seed(rep_index)
M=m*n
beta_true=rnorm(p,sd=kappa1)
X=matrix(rnorm(p*n,mean=0,sd=sqrt(1/p)),nc=p)
Xstar=matrix(rnorm(p*M,mean=0,sd=sqrt(1/p)),nc=p)
y1=rbinom(n,1, 1/(1+exp(-X%*%beta_true)))
lambda=kappa2*(sqrt(1-kesee^2))
beta0=kesee*kappa2/kappa1*beta_true+lambda*rnorm(p)
y2=rbinom(M,1, 1/(1+exp(-Xstar%*%beta0)))
fit_mle_real<-glmnet(X,y1,family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
fit_mle_hist<-glmnet(Xstar,y2,family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
inner_prod=c(inner_prod,sum(as.numeric(fit_mle_real$beta)*as.numeric(fit_mle_hist$beta))/p)
}
mean(inner_prod)/0.1264642/0.1156684/kappa1/kappa2
rm(list=ls())
library(parallel)
numCores=detectCores()
library(sigmoid)
library(cubature) # numerical integration
library(mvtnorm)
library(rootSolve) # root finding by built in function
rho<-function(x){
if(x>10) return(10.00005)
log(1+exp(x))
}
drho<-function(x){
sigmoid(x)
}
d2rho<-function(x){
sigmoid(x)*(1-sigmoid(x))
}
prox_rho<-function(v,t){          # argmin_x t*pho(x)+|v-x|^2/2
return(optim(0,function(x){(x-v)^2/2+t*rho(x)},method = "Brent",lower = -1000000000,upper = 1000000000)$par)
newf<-function(xx){
xx+t*drho(xx)-v
}
uniroot(newf,c(-10000,10000))$root
}
f1<-function(Z,kappa_,alpha,sigma,gamma_){
term1=kappa_*alpha*Z[1]+sigma*Z[2]
dmvnorm(Z)* drho(-kappa_*Z[1])*(term1-prox_rho(term1,gamma_))^2
}
f2<-function(Z,kappa_,alpha,sigma,gamma_){
dmvnorm(Z)* d2rho(-kappa_*Z[1])*prox_rho(kappa_*alpha*Z[1]+sigma*Z[2],gamma_)
}
f3<-function(Z,kappa_,alpha,sigma,gamma_){
dmvnorm(Z)* 2*drho(-kappa_*Z[1])/(1+gamma_*d2rho(prox_rho(kappa_*alpha*Z[1]+sigma*Z[2],gamma_)))
}
S_minus_V<-function(v,delta=3,kappa_=1,lambda=0.8,intergal_limit=50){
y<- numeric(3)
sigma=v[1];alpha=v[2];gamma_=v[3]
y[1]=sqrt(2*delta*hcubature(f1,lowerLimit = c(-intergal_limit,-intergal_limit),
upperLimit = c(intergal_limit,intergal_limit),kappa_,alpha,sigma,gamma_)$integral)-sigma
y[2]=-2*delta*hcubature(f2,lowerLimit = c(-intergal_limit,-intergal_limit),
upperLimit = c(intergal_limit,intergal_limit),kappa_,alpha,sigma,gamma_)$integral-alpha
y[3]=(hcubature(f3,lowerLimit = c(-intergal_limit,-intergal_limit),
upperLimit = c(intergal_limit,intergal_limit),kappa_,alpha,sigma,gamma_)$integral-1+1/delta)/lambda  -gamma_
return(y)
}
delta=1/2
############### ridge with lambda=1/p
kappa1_seq=seq(0.2,2,by=0.05)
result_list_ridge<-mclapply(1:length(kappa1_seq),function(jj){
kappa1=kappa1_seq[jj]
v_init=c(0.4314904, 0.1264642, 1.7243201)
multiroot(S_minus_V,v_init ,delta=delta,kappa_=kappa1,lambda=1,intergal_limit=20)$root
},mc.cores=numCores)
solution_three=matrix(0,nr=length(kappa1_seq),nc=3)
for(i in 1:length(kappa1_seq)){
solution_three[i,]=result_list_ridge[[i]]
}
theory_class_error=function_return_class_err_ridge(solution_three,kappa1_seq)
# compute the classification error based on solution_three
return_class_err<-function(sigma,kappa1,alpha){
vv=numeric()
for(j in 1:10){
Z_matrix=matrix(rnorm(10000),nc=2)
vv[j]=mean(sapply(1:5000,function(ii){
Z1=Z_matrix[ii,1]
Z2=Z_matrix[ii,2]
drho(kappa1*Z2)*(sigma*Z1+alpha*kappa1*Z2<0)+(1-drho(kappa1*Z2))*(sigma*Z1+alpha*kappa1*Z2>0)
}))
}
mean(vv)
}
function_return_class_err_ridge=function(solution_three,kappa1_seq){
sapply(1:nrow(solution_three),function(ii){
kappa1=kappa1_seq[ii]
sigma=solution_three[ii,1]
alpha=solution_three[ii,2]
return_class_err(sigma,kappa1,alpha)
})
}
theory_class_error=function_return_class_err_ridge(solution_three,kappa1_seq)
theory_class_error
empirical_kappa_hat_cv_onetime<-function(X,y1,theory_class_error,kappa1_seq,reample_time=50){
cl_error_rep=rep(0,50)
n=nrow(X)
p=ncol(X)
for(resample_index in 1:50){
test_index=sample(1:n,n/4,replace = FALSE)
fit_ridge<-glmnet(X[-test_index,],y1[-test_index],family = "binomial",alpha=0,lambda = 1/p,intercept = FALSE,standardize = FALSE)
betahat_ridge=as.numeric(fit_ridge$beta)
ypred_ridge=1*(drho(X[test_index,]%*%betahat_ridge)>0.5)
cl_error_rep[resample_index]=mean(ypred_ridge!=y1[test_index])
}
em_err=mean(cl_error_rep)
kappa1_seq[which.min(abs(em_err-theory_class_error))]
}
rep_each_kappa1_function<-function(indexx){
kappa1=kappa1_seq[indexx]
p=600
delta=2/3
n=p*delta
estimated_kappa_hat=rep(0,30)
for(rep_index in 1:30){
set.seed(rep_index)
beta_true=rnorm(p,sd=kappa1)
X=matrix(rnorm(p*n,mean=0,sd=sqrt(1/p)),nc=p)
y1=rbinom(n,1, 1/(1+exp(-X%*%beta_true)))
estimated_kappa_hat[rep_index]= empirical_kappa_hat_cv_onetime(X,y1,theory_class_error,kappa1_seq)
}
estimated_kappa_hat
}
result_list<-mclapply(1:length(kappa1_seq),rep_each_kappa1_function,mc.cores=numCores)
result_matrix=do.call(rbind,result_list)
rep_each_kappa1_function(1)
# now the question is: how to estimate classification error use data
library(glmnet)
result_list<-mclapply(1:length(kappa1_seq),rep_each_kappa1_function,mc.cores=numCores)
result_matrix=do.call(rbind,result_list)
result_matrix
plot(kappa1_seq,apply(result_matrix,1,mean),main = "delta = 2/3 ",ylab = "estimated kappa1")
lines(kappa1_seq,kappa1_seq,col=2)
save(result_matrix,file = "result_matrix_large_p")
save(theory_class_error,file = "theory_class_error_large_p")
load("result_matrix_5")
result_matrix
############### ridge with lambda=1/p
kappa1_seq=seq(0.2,2,by=0.05)
kappa1_seq
length(kappa1_seq)
result_matrix
hist(result_matrix[15,])
kappa1_seq[15]
library("glmhd")
prox_op
rho_prime_logistic()
rho_prime_logistic
setwd("~/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/CGMT_cat/numerical_study/estimation_of_similarity")
setwd("~/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/CGMT_cat/numerical_study/estimation_of_signal_strength/create_dictionary")
setwd("~/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/CGMT_cat/numerical_study/estimation_of_similarity")
